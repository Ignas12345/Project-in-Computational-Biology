{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Edge Net"
      ],
      "metadata": {
        "id": "xWhlC6pLubgT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gXXzfCcuauC",
        "outputId": "c11a1a7b-5167-4e2a-8e97-25284201dd66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'EdgeNet' already exists and is not an empty directory.\n",
            "mv: cannot stat 'EdgeNet/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Nikronic/EdgeNet.git\n",
        "!mv EdgeNet/* ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% import library\n",
        "from EdgeNet import EdgeNet\n",
        "from torchvision.transforms import Compose, ToPILImage, ToTensor, RandomResizedCrop, RandomRotation, \\\n",
        "    RandomHorizontalFlip, Normalize\n",
        "from utils.preprocess import *\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from utils.loss import EdgeLoss\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import argparse\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# %% config parser\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--txt\", help='path to the text file', default='filelist.txt')\n",
        "parser.add_argument(\"--img\", help='path to the images tar archive (uncompressed) or extracted folder', default='data')\n",
        "parser.add_argument(\"--txt_t\", help='path to the text file of test set', default='filelist.txt')\n",
        "parser.add_argument(\"--img_t\", help='path to the images tar archive (uncompressed) or extracted folder of test set',\n",
        "                    default='data')\n",
        "parser.add_argument(\"--bs\", help='int number as batch size', default=3, type=int)\n",
        "parser.add_argument(\"--es\", help='int number as number of epochs', default=100, type=int)\n",
        "parser.add_argument(\"--nw\", help='number of workers (1 to 8 recommended)', default=4, type=int)\n",
        "parser.add_argument(\"--lr\", help='learning rate of optimizer (=0.0001)', default=0.00001, type=float)\n",
        "parser.add_argument(\"--cudnn\", help='enable(1) cudnn.benchmark or not(0)', default=0, type=int)\n",
        "parser.add_argument(\"--pm\", help='enable(1) pin_memory or not(0)', default=0, type=int)\n",
        "#cia dar veikia\n",
        "args = parser.parse_args([])\n",
        "#cia nebeveikia\n",
        "cudnn.benchmark = False\n",
        "pin_memory = False\n",
        "cudnn.benchmark = True if args.cudnn == 1 else False\n",
        "pin_memory = True if args.cudnn == 1 else False\n",
        "\n",
        "# %% define data sets and their loaders\n",
        "custom_transforms = Compose([\n",
        "    RandomResizedCrop(size=224, scale=(0.8, 1.2)),\n",
        "    RandomRotation(degrees=(-30, 30)),\n",
        "    RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    RandomNoise(p=0.5, mean=0, std=0.1)])\n",
        "\n",
        "train_dataset = PlacesDataset(txt_path=args.txt,\n",
        "                              img_dir=args.img,\n",
        "                              transform=custom_transforms)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=args.bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=args.nw,\n",
        "                          pin_memory=pin_memory)\n",
        "\n",
        "test_dataset = PlacesDataset(txt_path=args.txt_t,\n",
        "                             img_dir=args.img_t,\n",
        "                             transform=ToTensor())\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=128,\n",
        "                         shuffle=False,\n",
        "                         num_workers=0,\n",
        "                         pin_memory=False)\n",
        "\n",
        "\n",
        "# %% initialize network, loss and optimizer\n",
        "def init_weights(m):\n",
        "    \"\"\"\n",
        "    Initialize weights of layers using Kaiming Normal (He et al.) as argument of \"Apply\" function of\n",
        "    \"nn.Module\"\n",
        "\n",
        "    :param m: Layer to initialize\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "        m.bias.data.fill_(0.0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):  # reference: https://github.com/pytorch/pytorch/issues/12259\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "# %% train model\n",
        "def train_model(net, data_loader, optimizer, criterion, epochs=10):\n",
        "    \"\"\"\n",
        "    Train model\n",
        "\n",
        "    :param net: Parameters of defined neural network\n",
        "    :param data_loader: A data loader object defined on train data set\n",
        "    :param epochs: Number of epochs to train model\n",
        "    :param optimizer: Optimizer to train network\n",
        "    :param criterion: The loss function to minimize by optimizer\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "            y_descreen = data['y_descreen']\n",
        "            y_e = data['y_edge']\n",
        "\n",
        "            y_descreen = y_descreen.to(device)\n",
        "            y_e = y_e.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(y_descreen)\n",
        "            loss = criterion(outputs, y_e.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss))\n",
        "            running_loss = 0.0\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "# %% test\n",
        "def test_model(net, data_loader):\n",
        "    \"\"\"\n",
        "    Return loss on test\n",
        "\n",
        "    :param net: The trained NN network\n",
        "    :param data_loader: Data loader containing test set\n",
        "    :return: Print loss value over test set in console\n",
        "    \"\"\"\n",
        "\n",
        "    net.eval()\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            y_descreen = data['y_descreen']\n",
        "            y_e = data['y_edge']\n",
        "\n",
        "            y_descreen = y_descreen.to(device)\n",
        "            y_e = y_e.to(device)\n",
        "            outputs = net(y_descreen)\n",
        "            loss = criterion(outputs, y_e)\n",
        "            running_loss += loss\n",
        "            print('loss: %.3f' % running_loss)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def show_batch_image(image_batch):\n",
        "    \"\"\"\n",
        "    Show a sample grid image which contains some sample of test set result\n",
        "\n",
        "    :param image_batch: The output batch of test set\n",
        "    :return: PIL image of all images of the input batch\n",
        "    \"\"\"\n",
        "\n",
        "    to_pil = ToPILImage()\n",
        "    fs = []\n",
        "    for i in range(len(image_batch)):\n",
        "        img = to_pil(image_batch[i].cpu())\n",
        "        fs.append(img)\n",
        "    x, y = fs[0].size\n",
        "    ncol = int(np.ceil(np.sqrt(len(image_batch))))\n",
        "    nrow = int(np.ceil(np.sqrt(len(image_batch))))\n",
        "    cvs = Image.new('RGB', (x * ncol, y * nrow))\n",
        "    for i in range(len(fs)):\n",
        "        px, py = x * int(i / nrow), y * (i % nrow)\n",
        "        cvs.paste((fs[i]), (px, py))\n",
        "    cvs.save('out.png', format='png')\n",
        "    cvs.show()\n",
        "\n",
        "\n",
        "# %% run model\n",
        "criterion = EdgeLoss().to(device)\n",
        "edgenet = EdgeNet().to(device)\n",
        "optimizer = optim.Adam(edgenet.parameters(), lr=args.lr)\n",
        "#edgenet.apply(init_weights)#used for initilization - I want to try to use old parameters\n",
        "edgenet.load_state_dict(torch.load('model_parameters_3')) # delete this if you want to start from random initialization\n",
        "train_model(edgenet, train_loader, optimizer, criterion, epochs=args.es)\n",
        "show_batch_image(test_model(edgenet, test_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntSqzKvyuo4D",
        "outputId": "8973274c-2333-4d0a-de55-fbb4ecce9445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: 0.556\n",
            "[1,     2] loss: 0.510\n",
            "[1,     3] loss: 0.536\n",
            "[2,     1] loss: 0.448\n",
            "[2,     2] loss: 0.592\n",
            "[2,     3] loss: 0.533\n",
            "[3,     1] loss: 0.490\n",
            "[3,     2] loss: 0.499\n",
            "[3,     3] loss: 0.573\n",
            "[4,     1] loss: 0.508\n",
            "[4,     2] loss: 0.505\n",
            "[4,     3] loss: 0.639\n",
            "[5,     1] loss: 0.620\n",
            "[5,     2] loss: 0.559\n",
            "[5,     3] loss: 0.569\n",
            "[6,     1] loss: 0.411\n",
            "[6,     2] loss: 0.576\n",
            "[6,     3] loss: 0.528\n",
            "[7,     1] loss: 0.534\n",
            "[7,     2] loss: 0.621\n",
            "[7,     3] loss: 0.490\n",
            "[8,     1] loss: 0.570\n",
            "[8,     2] loss: 0.512\n",
            "[8,     3] loss: 0.591\n",
            "[9,     1] loss: 0.569\n",
            "[9,     2] loss: 0.545\n",
            "[9,     3] loss: 0.535\n",
            "[10,     1] loss: 0.462\n",
            "[10,     2] loss: 0.519\n",
            "[10,     3] loss: 0.634\n",
            "[11,     1] loss: 0.533\n",
            "[11,     2] loss: 0.546\n",
            "[11,     3] loss: 0.541\n",
            "[12,     1] loss: 0.631\n",
            "[12,     2] loss: 0.637\n",
            "[12,     3] loss: 0.456\n",
            "[13,     1] loss: 0.530\n",
            "[13,     2] loss: 0.503\n",
            "[13,     3] loss: 0.493\n",
            "[14,     1] loss: 0.629\n",
            "[14,     2] loss: 0.486\n",
            "[14,     3] loss: 0.426\n",
            "[15,     1] loss: 0.568\n",
            "[15,     2] loss: 0.530\n",
            "[15,     3] loss: 0.534\n",
            "[16,     1] loss: 0.547\n",
            "[16,     2] loss: 0.377\n",
            "[16,     3] loss: 0.466\n",
            "[17,     1] loss: 0.592\n",
            "[17,     2] loss: 0.483\n",
            "[17,     3] loss: 0.628\n",
            "[18,     1] loss: 0.459\n",
            "[18,     2] loss: 0.634\n",
            "[18,     3] loss: 0.547\n",
            "[19,     1] loss: 0.454\n",
            "[19,     2] loss: 0.496\n",
            "[19,     3] loss: 0.630\n",
            "[20,     1] loss: 0.382\n",
            "[20,     2] loss: 0.529\n",
            "[20,     3] loss: 0.589\n",
            "[21,     1] loss: 0.396\n",
            "[21,     2] loss: 0.550\n",
            "[21,     3] loss: 0.564\n",
            "[22,     1] loss: 0.637\n",
            "[22,     2] loss: 0.539\n",
            "[22,     3] loss: 0.569\n",
            "[23,     1] loss: 0.467\n",
            "[23,     2] loss: 0.647\n",
            "[23,     3] loss: 0.491\n",
            "[24,     1] loss: 0.619\n",
            "[24,     2] loss: 0.467\n",
            "[24,     3] loss: 0.581\n",
            "[25,     1] loss: 0.509\n",
            "[25,     2] loss: 0.497\n",
            "[25,     3] loss: 0.580\n",
            "[26,     1] loss: 0.584\n",
            "[26,     2] loss: 0.481\n",
            "[26,     3] loss: 0.378\n",
            "[27,     1] loss: 0.522\n",
            "[27,     2] loss: 0.480\n",
            "[27,     3] loss: 0.467\n",
            "[28,     1] loss: 0.447\n",
            "[28,     2] loss: 0.562\n",
            "[28,     3] loss: 0.486\n",
            "[29,     1] loss: 0.527\n",
            "[29,     2] loss: 0.564\n",
            "[29,     3] loss: 0.549\n",
            "[30,     1] loss: 0.531\n",
            "[30,     2] loss: 0.506\n",
            "[30,     3] loss: 0.647\n",
            "[31,     1] loss: 0.547\n",
            "[31,     2] loss: 0.472\n",
            "[31,     3] loss: 0.567\n",
            "[32,     1] loss: 0.537\n",
            "[32,     2] loss: 0.639\n",
            "[32,     3] loss: 0.632\n",
            "[33,     1] loss: 0.434\n",
            "[33,     2] loss: 0.628\n",
            "[33,     3] loss: 0.589\n",
            "[34,     1] loss: 0.427\n",
            "[34,     2] loss: 0.525\n",
            "[34,     3] loss: 0.529\n",
            "[35,     1] loss: 0.585\n",
            "[35,     2] loss: 0.471\n",
            "[35,     3] loss: 0.531\n",
            "[36,     1] loss: 0.474\n",
            "[36,     2] loss: 0.499\n",
            "[36,     3] loss: 0.549\n",
            "[37,     1] loss: 0.537\n",
            "[37,     2] loss: 0.487\n",
            "[37,     3] loss: 0.502\n",
            "[38,     1] loss: 0.455\n",
            "[38,     2] loss: 0.520\n",
            "[38,     3] loss: 0.544\n",
            "[39,     1] loss: 0.533\n",
            "[39,     2] loss: 0.626\n",
            "[39,     3] loss: 0.526\n",
            "[40,     1] loss: 0.643\n",
            "[40,     2] loss: 0.618\n",
            "[40,     3] loss: 0.458\n",
            "[41,     1] loss: 0.631\n",
            "[41,     2] loss: 0.430\n",
            "[41,     3] loss: 0.511\n",
            "[42,     1] loss: 0.496\n",
            "[42,     2] loss: 0.546\n",
            "[42,     3] loss: 0.569\n",
            "[43,     1] loss: 0.366\n",
            "[43,     2] loss: 0.592\n",
            "[43,     3] loss: 0.504\n",
            "[44,     1] loss: 0.573\n",
            "[44,     2] loss: 0.618\n",
            "[44,     3] loss: 0.493\n",
            "[45,     1] loss: 0.433\n",
            "[45,     2] loss: 0.563\n",
            "[45,     3] loss: 0.575\n",
            "[46,     1] loss: 0.621\n",
            "[46,     2] loss: 0.477\n",
            "[46,     3] loss: 0.591\n",
            "[47,     1] loss: 0.494\n",
            "[47,     2] loss: 0.564\n",
            "[47,     3] loss: 0.456\n",
            "[48,     1] loss: 0.442\n",
            "[48,     2] loss: 0.531\n",
            "[48,     3] loss: 0.474\n",
            "[49,     1] loss: 0.533\n",
            "[49,     2] loss: 0.596\n",
            "[49,     3] loss: 0.415\n",
            "[50,     1] loss: 0.377\n",
            "[50,     2] loss: 0.487\n",
            "[50,     3] loss: 0.493\n",
            "[51,     1] loss: 0.484\n",
            "[51,     2] loss: 0.624\n",
            "[51,     3] loss: 0.546\n",
            "[52,     1] loss: 0.482\n",
            "[52,     2] loss: 0.649\n",
            "[52,     3] loss: 0.571\n",
            "[53,     1] loss: 0.465\n",
            "[53,     2] loss: 0.516\n",
            "[53,     3] loss: 0.577\n",
            "[54,     1] loss: 0.391\n",
            "[54,     2] loss: 0.509\n",
            "[54,     3] loss: 0.488\n",
            "[55,     1] loss: 0.502\n",
            "[55,     2] loss: 0.456\n",
            "[55,     3] loss: 0.363\n",
            "[56,     1] loss: 0.518\n",
            "[56,     2] loss: 0.569\n",
            "[56,     3] loss: 0.507\n",
            "[57,     1] loss: 0.553\n",
            "[57,     2] loss: 0.456\n",
            "[57,     3] loss: 0.585\n",
            "[58,     1] loss: 0.515\n",
            "[58,     2] loss: 0.582\n",
            "[58,     3] loss: 0.482\n",
            "[59,     1] loss: 0.628\n",
            "[59,     2] loss: 0.475\n",
            "[59,     3] loss: 0.392\n",
            "[60,     1] loss: 0.642\n",
            "[60,     2] loss: 0.568\n",
            "[60,     3] loss: 0.577\n",
            "[61,     1] loss: 0.391\n",
            "[61,     2] loss: 0.561\n",
            "[61,     3] loss: 0.573\n",
            "[62,     1] loss: 0.451\n",
            "[62,     2] loss: 0.438\n",
            "[62,     3] loss: 0.585\n",
            "[63,     1] loss: 0.525\n",
            "[63,     2] loss: 0.472\n",
            "[63,     3] loss: 0.473\n",
            "[64,     1] loss: 0.545\n",
            "[64,     2] loss: 0.535\n",
            "[64,     3] loss: 0.629\n",
            "[65,     1] loss: 0.596\n",
            "[65,     2] loss: 0.466\n",
            "[65,     3] loss: 0.477\n",
            "[66,     1] loss: 0.555\n",
            "[66,     2] loss: 0.396\n",
            "[66,     3] loss: 0.455\n",
            "[67,     1] loss: 0.531\n",
            "[67,     2] loss: 0.483\n",
            "[67,     3] loss: 0.569\n",
            "[68,     1] loss: 0.587\n",
            "[68,     2] loss: 0.562\n",
            "[68,     3] loss: 0.585\n",
            "[69,     1] loss: 0.453\n",
            "[69,     2] loss: 0.405\n",
            "[69,     3] loss: 0.426\n",
            "[70,     1] loss: 0.433\n",
            "[70,     2] loss: 0.532\n",
            "[70,     3] loss: 0.453\n",
            "[71,     1] loss: 0.531\n",
            "[71,     2] loss: 0.644\n",
            "[71,     3] loss: 0.512\n",
            "[72,     1] loss: 0.543\n",
            "[72,     2] loss: 0.398\n",
            "[72,     3] loss: 0.475\n",
            "[73,     1] loss: 0.389\n",
            "[73,     2] loss: 0.522\n",
            "[73,     3] loss: 0.420\n",
            "[74,     1] loss: 0.642\n",
            "[74,     2] loss: 0.470\n",
            "[74,     3] loss: 0.556\n",
            "[75,     1] loss: 0.532\n",
            "[75,     2] loss: 0.522\n",
            "[75,     3] loss: 0.466\n",
            "[76,     1] loss: 0.460\n",
            "[76,     2] loss: 0.484\n",
            "[76,     3] loss: 0.559\n",
            "[77,     1] loss: 0.483\n",
            "[77,     2] loss: 0.479\n",
            "[77,     3] loss: 0.439\n",
            "[78,     1] loss: 0.565\n",
            "[78,     2] loss: 0.500\n",
            "[78,     3] loss: 0.477\n",
            "[79,     1] loss: 0.455\n",
            "[79,     2] loss: 0.637\n",
            "[79,     3] loss: 0.593\n",
            "[80,     1] loss: 0.552\n",
            "[80,     2] loss: 0.519\n",
            "[80,     3] loss: 0.436\n",
            "[81,     1] loss: 0.426\n",
            "[81,     2] loss: 0.456\n",
            "[81,     3] loss: 0.628\n",
            "[82,     1] loss: 0.583\n",
            "[82,     2] loss: 0.639\n",
            "[82,     3] loss: 0.649\n",
            "[83,     1] loss: 0.549\n",
            "[83,     2] loss: 0.639\n",
            "[83,     3] loss: 0.584\n",
            "[84,     1] loss: 0.468\n",
            "[84,     2] loss: 0.522\n",
            "[84,     3] loss: 0.495\n",
            "[85,     1] loss: 0.549\n",
            "[85,     2] loss: 0.471\n",
            "[85,     3] loss: 0.493\n",
            "[86,     1] loss: 0.433\n",
            "[86,     2] loss: 0.449\n",
            "[86,     3] loss: 0.642\n",
            "[87,     1] loss: 0.515\n",
            "[87,     2] loss: 0.483\n",
            "[87,     3] loss: 0.461\n",
            "[88,     1] loss: 0.478\n",
            "[88,     2] loss: 0.555\n",
            "[88,     3] loss: 0.546\n",
            "[89,     1] loss: 0.625\n",
            "[89,     2] loss: 0.632\n",
            "[89,     3] loss: 0.492\n",
            "[90,     1] loss: 0.628\n",
            "[90,     2] loss: 0.451\n",
            "[90,     3] loss: 0.478\n",
            "[91,     1] loss: 0.483\n",
            "[91,     2] loss: 0.539\n",
            "[91,     3] loss: 0.538\n",
            "[92,     1] loss: 0.589\n",
            "[92,     2] loss: 0.570\n",
            "[92,     3] loss: 0.556\n",
            "[93,     1] loss: 0.492\n",
            "[93,     2] loss: 0.522\n",
            "[93,     3] loss: 0.488\n",
            "[94,     1] loss: 0.545\n",
            "[94,     2] loss: 0.364\n",
            "[94,     3] loss: 0.627\n",
            "[95,     1] loss: 0.487\n",
            "[95,     2] loss: 0.520\n",
            "[95,     3] loss: 0.569\n",
            "[96,     1] loss: 0.576\n",
            "[96,     2] loss: 0.569\n",
            "[96,     3] loss: 0.566\n",
            "[97,     1] loss: 0.514\n",
            "[97,     2] loss: 0.454\n",
            "[97,     3] loss: 0.497\n",
            "[98,     1] loss: 0.518\n",
            "[98,     2] loss: 0.479\n",
            "[98,     3] loss: 0.417\n",
            "[99,     1] loss: 0.565\n",
            "[99,     2] loss: 0.438\n",
            "[99,     3] loss: 0.528\n",
            "[100,     1] loss: 0.522\n",
            "[100,     2] loss: 0.467\n",
            "[100,     3] loss: 0.507\n",
            "Finished Training\n",
            "loss: 0.341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(edgenet.state_dict(), 'model_parameters_4')"
      ],
      "metadata": {
        "id": "uqaz_Yno2NyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}